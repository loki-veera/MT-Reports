%!TEX root = ../report.tex

\begin{document}
    \chapter{State of the Art}
    This chapter will discuss the 3D LiDAR datasets available and classify them based on the type of acquisition.
    Also, we will discuss the existing 3D semantic segmentation models, uncertainty estimation methods and Out-of-Distribution (OOD) detection methods available.
    \section{3D LiDAR Datasets}
    %Since the advent of AlexNet \cite{alexnet}, deeplearning has been in the rise. 
    %Deep convolutional neural networks are the desired option for image analysis tasks such as classification, detection and segmentation.
    %These deep convolutional networks are successful especially because of two reasons. 
    %First is the architectures and their paralellisation, allowing them to train millions of images on a GPU.
    %Second reason is the development of huge public benchmark datasets such as Imagenet \cite{imagenet}, Pascal VOC \cite{pascalvoc} and COCO \cite{coco} datasets
    %Although deep convolutional networks are huge success in image analysis tasks, they perform poorly on 3D point cloud data.
    This section will discuss the available LiDAR datasets for the 3D semantic segmentation task and classify the datasets based on acquisition methods as in \cite{survey3d}.
    Before the survey on available datasets, we would like to introduce LiDAR and the types of LiDAR.
    LiDAR is a time of flight (ToF) sensor where an optical pulse is emitted by the source and the reflected pulse is detected back by the source.
    The time between the emission and detection is used to calculate the distance of the object.
    There are two types of LiDAR, based on steering of beams.
    The first type is the mechanical LiDARs where an optical assembly is set over a gear operated rotating device and pulses are constantly emitted and detected.
    Mechanical LiDARs provide $360^{\circ}$ field of view.
    The second type is the solid-state LiDARs where no rotating devices are present and have a very limited field of view. 
    A detailed analysis of LiDARs is presented in \cite{lidar}.


    Among 3D LiDAR datasets, \cite{survey3d} classifies the available public datasets into three classes based on the data acquisition process.
    They are \textit{sequential}, \textit{static} and \textit{synthetic} datasets.
    For a sequential dataset, the data is collected as a sequence of frames with a mechanical LiDAR mounted on top of an autonomous driving platform, as depicted in Figure \ref{fig:seq_data_lyft}.
    Sequential datasets can be defined as a collection of point cloud scans over a period of time.
    Figure~\ref{fig:example_sequential} represents an example point cloud from a sequential dataset.
    Most of the popular autonomous driving datasets are sequential. However, the sequential datasets come with a drawback of sparse points than static or synthetic datasets.
    \begin{figure}[h!]
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[scale=0.25]{images/sequential_lyft.png}
            \caption{}
            \label{fig:seq_data_lyft}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[scale=0.45]{images/SemanticKitti.pdf}
            \caption{}
            \label{fig:example_sequential}
        \end{subfigure}
        \caption{Image (a) depicting the LiDAR mounted on a vehicle to collect sequential data for the Lyft L5 dataset and 
        (b) depicts an example of sequential data from the SemanticKITTI dataset. Images are from \cite{Lyftl5} and \cite{Hu_2020_CVPR_Randla} respectively.}
    \end{figure}
    
    
    Static datasets consist of data collected from a stationary viewpoint by a terrestrial laser scanner.
    This kind of dataset captures the static information of the real world.
    Static datasets find their way into urban planning, augmented reality, and robotics applications. 
    Figure \ref{fig:tls} depicts a terrestrial laser scanner used to capture the static point cloud of an industrial environment, and Figure~\ref{fig:static_scan} represents an example point cloud for a static dataset.
    \begin{figure}[h!]
        \centering
        \begin{subfigure}{0.45\textwidth}
            \includegraphics[scale=0.45]{images/TLS.jpg}
            \caption{}
            \label{fig:tls}
        \end{subfigure}
        \begin{subfigure}{0.45\textwidth}
            \centering
            \includegraphics[scale=0.2]{images/sem3d_data/1.pdf}
            \caption{}
            \label{fig:static_scan}
        \end{subfigure}

        \caption{	Image (a) depicting the terrestrial laser scanner in an industrial environment to collect static data with the laser scanner mounted on a yellow tripod in the left corner of the floor and 
        (b) represents the scene of static data from the Semantic3D dataset. The first image is taken from \cite{tls}.}
       
    \end{figure}
    Typically static datasets produce highly dense point clouds leading to rich geometric representations.
    
    The last type of 3D LiDAR dataset is the synthetic dataset. 
    As the name suggests, these datasets are generated from computer simulation. 
    Figure \ref{fig:synthetic} depicts a simulated point cloud in a synthetic dataset called SynthCity.
    Even though synthetic datasets can be generated on a large scale at a low cost, they lack detail over point clouds recorded from the real world.
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.5]{images/synthcity.png}
        \caption{Illustration of a scene in synthetic dataset called SynthCity. Image taken from \cite{griffiths2019synthcity}}
        \label{fig:synthetic}
    \end{figure}
    
    The datasets belonging to each acquisition type are summed up in  Table \ref{table:3d_lidar_datasets_table}.
    Most of the datasets from Table \ref{table:3d_lidar_datasets_table} are taken from \cite{survey3d}. As a part of this study, additional new datasets were added to the list.
    The newly added datasets include DALES \cite{varney2020dales}, ScanObjectNN \cite{scanobejctnn} in static acquisition mode and AIO Drive \cite{Weng2020_AIODrive}, Toronto3D \cite{tan2020toronto3d} are additions in the sequential mode.
    % \cite{survey3d} also classifies GTAV \textcolor{red}{\textbf{cite}} dataset as synthetic 3D LiDAR but the corresponding paper doesn't report any LiDAR dataset and proposed only 2D dataset for segmentation.
    The limited number of datasets in 3D LiDAR allowed us to study the characteristics of each dataset, such as each class, data distribution and features of the point cloud. 
    %%%%% It is summarized in Table \textcolor{red}{\textbf{ref}} in Appendix \textcolor{red}{\textbf{chapter number}}
    %%\section{TODO}
    %%\begin{itemize}
    %%     \item[$\bullet$] Explain the table
    %%    \item[$\bullet$] Discuss why SemanticKITTI and Semantic3D are of our interest     
    %%\end{itemize}
    
    \begin{table}[h!]
        %\centering
        \begin{tabular}{c|c|c|c|c|c}%|c}
            \hline
            % acquisition type, dataset, frames, #points, classes, I/O, year
            acquisition mode & dataset & frames & total points (in million) & classes & scene type \\  % & pub. year \\
            \hline
            \multirow{7}{*}{static} & Oakland \cite{oakland} & 17 & 1.6 &  44 & outdoor \\ % & 2009 \\ 
                                    & Paris-lille-3D \cite{roynard2018paris} & 3 & 143 & 50 & outdoor \\ % & 2018 \\
                                    & Paris-rue-Madame \cite{paris_rue_madame} & 2 & 20 & 17 & outdoor \\ %& 2014 \\
                                    & S3DIS \cite{Armeni_2016_CVPR_S3DIS} & 5 & 215 & 12 & indoor \\ %& 2016 \\
                                    & ScanObjectNN \cite{scanobejctnn} & - & - & 15 & indoor \\ %& 2019 \\
                                    & Semantic3D \cite{hackel2017semantic3d} & 30 & 4009 & 8 & outdoor \\ %& 2017 \\
                                    & TerraMobilita/IQmulus \cite{TerraMobilita} & 10 & 12 & 15 & outdoor\\ % & 2015 \\
                                    & TUM City Campus \cite{gehrung2017approach_tum_campus} & 631 & 41 & 8 & outdoor\\ % & 2016 \\
                                    & DALES \cite{varney2020dales} & 40 (tiles) & 492 & 8 & outdoor\\ % & 2021\\
            \hline
            \multirow{7}{*}{sequential} & A2D2 \cite{geyer2020a2d2} & 41277 & 1238 & 38 & outdoor\\ % & 2020\\
                                        & AIO Drive \cite{Weng2020_AIODrive} & 100& - & 23 & outdoor\\ % & 2020\\
                                        & KITTI-360 \cite{Xie_2016_CVPR_KITTI_360} & 100K & 18000 & 19 & outdoor\\ %& 2020\\
                                        & nuScenes-lidarseg \cite{caesar2020nuscenes} & 40000 & 1400 & 32& outdoor\\ % & 2020\\
                                        & PandaSet \cite{PandaSet} & 16000 & 1844 & 37 & outdoor \\ %& 2020\\
                                        & SemanticKITTI \cite{Behley_2019_ICCV} & 43552 & 4549 & 28 & outdoor \\ %& 2019\\
                                        & SemanticPOSS \cite{pan2020semanticposs} & 2988 & 216 & 14 & outdoor \\ %& 2020\\
                                        & Sydney Urban \cite{de2013unsupervised} & 631 & - & 26 & outdoor\\ % & 2013\\
                                        & Toronto-3D \cite{tan2020toronto3d} & 4 & 78.3& 8& outdoor\\ % & 2020\\
    
            \hline
            \multirow{1}{*}{synthetic}  & SynthCity \cite{griffiths2019synthcity} & 75000 & 367.9 & 9 & outdoor \\ %& 2019\\
            \hline
        \end{tabular}
        \caption{3D LiDAR datasets classified based on the acquisition type. Table updated from \cite{survey3d}}
        \label{table:3d_lidar_datasets_table}
    \end{table}
    
    We chose the Semantic3D dataset as the in-distribution (ID) training dataset from the available datasets. 
    S3DIS is chosen as an out-of-distribution (OOD) dataset.
    Chapter~\ref{ch:benchmark} represents the detailed discussion on datasets and benchmarking.

    \section{3D Semantic Segmentation Models}

    This section will discuss the methods available for 3D semantic segmentation.
    The discussion includes a brief peek into traditional 3D semantic segmentation methods and the study of deep learning methods for 3D point cloud segmentation.

\subsection{Traditional Approach}
Traditional methods involve a complex feature extraction and passing these features to a classification algorithm such as Support Vector Machines or Random Forests to classify each point in the point cloud.
Some of the traditional methods include segmentation from edge information \cite{bhanu1986range}, construction of complex graph pyramids \cite{koster}.
3D Hough transforms as in \cite{vosselman20013d} and application of RANSAC \cite{schnabel2007efficient} and \cite{tarsha2007hough}.
The performance of traditional methods is far less compared to the performance of DNNs.
This is because the DNN's are large and extract better features on their own.
%These traditional methods are outdated as DNNs proved to be better at feature extraction.

\subsection{Deep Learning Approach}
\label{sec:dl_approach}
Deep learning-based models are efficient at segmentation and can be divided into three types.
The first type includes the point-based models where the model directly feeds on the 3D point cloud.
Then the second type includes projection-based models where the model takes in projected points into a 2D image, either a range image or a bird's eye view.
The final type of model includes the use of graph neural networks.

Let us look briefly into the point-based methods. Point-based methods usually utilize fully connected layers or traditional convolutional layers.
A major revolution in Deep learning based 3D semantic segmentation came after PointNet proposed in \cite{Qi_2017_CVPR_pointnet}.
Pointnet applied Multi-Layer Perceptrons (MLP) to extract a global feature vector from the unstructured point cloud and then classify it using fully connected layers.
Since Pointnet can only extract local features, Pointnet++ proposed in \cite{qi2017pointnet++} applies Pointnet over the point cloud iteratively to extract features at a global scale and then classifies.
Even though the performance of Pointnet and Pointnet++ are weak in terms of segmentation, they served as a backbone for new architectures such as RandLA-Net proposed in \cite{Hu_2020_CVPR_Randla}.
Similar to PointNet, RandLA-Net employs MLP for feature extraction. These features are passed through attention pooling layer and then ouput of attention pooling to classification layers.
 Section~\ref{sec:meth_randla} describes in detail RandLA-Net.
It is one of the best-performing model with few parameters across Static and Sequential datasets.

The other flavours of point-based methods include projection onto a d-dimensional lattice or making the point cloud structured.
SPLATNet \cite{Su_2018_CVPR_splatnet} utilizes Bilateral Convolutional layers to project data on a d-dimensional lattice and then classify it.
Whereas LatticeNet proposed in \cite{rosu2019latticenet} projects the data onto sparse lattice, then classifies and learns the reprojection to 3D using a novel DeformSlice module.
Even though SPLATNet and LatticeNet use projection; they are grouped under point-based models because the model learns these projections.
\cite{spvnas} proposed SPVNAS where the point cloud is voxelized, classified, and devoxelized.
Finally, \cite{Tatarchenko_2018_CVPR_tangconv} proposed Tangent Convolution similar to standard convolutions but with extra multiplications.

In projection-based models, let us discuss the models that project the data onto a 2D range image.
The first of this kind is SqueezeSeg, proposed in \cite{Sequeseseg_2018}. It employs SqueezeNet for 2D segmentation and then reprojects the segmented range image to 3D using Recurrent Conditional Random Fields.
Since the performance of SqueezeSeg is weak, later versions of SqueezeSeg, such as SqueezeSegV2 proposed in \cite{SqueezeSegv2} and SqueezeSegV3 as in \cite{xu2020squeezesegv3} employ Context Aggregation Module and Spatially Adaptive Convolutions respectively, to improve performance relative to SqueezeSeg.
RangeNet proposed in \cite{Milioto2019} has three versions similar to SqueezeSeg, where the first two versions use the DarkNet21 and DarkNet53 model for 2D semantic segmentation and then reproject the data back to 3D.
The final version uses the DarkNet53 model with K-Nearest Neighbours (KNN) to reproject the 2D range image to 3D after segmentation.
Similarly, 3DMiniNet \cite{3Dmininet} and KPRNet \cite{kochanov2020kprnet} projects data to range image for segmentation with the  former using MiniNetV2 as the backbone model and later using ResNeXt-101 as the backbone.

We now discuss the projection-based models which involve bird eye view projection.
These models are relatively new to the field.
SalsaNet \cite{salsanet2020} and SalsaNext \cite{SalsaNext_2020} are the two models to employ bird eye view projection as input, and both use the ResNet
model as a backbone.
The latter one utilizes Lovasz-Softmax loss along with cross-entropy.
SalsaNext also involves the study of uncertainty by modelling SalsaNext as a Bayesian Neural Network.
PolarNet \cite{polarnet} and Cylinder3D \cite{zhu2020cylindrical} are recent models incorporating bird eye view projection into the segmentation pipeline.
The projections, such as 2D range image and bird's eye view projection, are performed prior to model input.

Table \ref{tab:model_relatedwork} illustrates the detailed summary of each model and the number of parameters.
Finally, graph neural networks \cite{dyn_graph_cnn} can also produce better segmentation results. However, this study is limited to standard deep learning models, so this study does not include graph neural networks.

    \section{Uncertainty Estimation Methods}
    \label{sec:ue_methods}
    This section will discuss existing methods to estimate uncertainty in deep neural networks.
    Here we divide the uncertainty estimation methods into Ensemble methods and Bayesian methods.
    The methods discussed here primarily estimate epistemic uncertainty. Only test time augmentation and Gaussian density models estimate aleatoric uncertainty.

    \cite{matias_uncertainty} argues that there are two kinds of uncertainties, they are epistemic and aleatoric uncertainty.
    Epistemic uncertainty is uncertainty in the model because of the lack of adequate training data or a proper model specification.
    \cite{matias_uncertainty} also argues that epistemic uncertainty is most helpful in OOD detection.
    This is because, during the training, model parameters train to a particular distribution of data and
    must produce high uncertainty estimates when they observe data from a different distribution.
    Aleatoric uncertainty is the uncertainty in the data like measurement noise.
    An example of aleatoric uncertainty in point cloud can be incorrect data from the LiDAR scanner collected on a foggy or rainy day, as fog or rain reflects most of the laser rays creating disturbances in the scan and limiting the range.
    Epistemic uncertainty can be reduced by increasing the size of training data or finding the proper model.
    Aleatoric uncertainty cannot be reduced as noise is the inherent property of the data.
    \subsection{Ensemble Methods}
    First proposed in \cite{lakshminarayanan2016simple}, Deep Ensembles are the most prominently used method for uncertainty estimation.
    They exploit the combinatory power of multiple models.
    Multiple model instances are initialized randomly, and the same input is fed into all the instances.
    The random initialization leads to slightly different optimization for each model instance. The final output scores from each model are combined by simple averaging.
    Deep Ensembles are known to improve the model's overall performance but come with a cost of computational complexity and resource intensiveness.
    Another advantage of using Deep Ensembles is the lowest correlation between the model instances as the training of the instances is done differently.
    These lower correlation figures lead to the diverse predictions from each model instance.
    A detailed explanation of Deep Ensembles can be found in Section~\ref{sec:meth_deepensembles}.

    Because of the higher computational complexity and resource intensiveness, there exist multiple flavours of Deep Ensembles, such as deep sub-ensembles \cite{deep_subensembles} and masksembles \cite{masksembles}.
    Deep sub-ensembles divide the network into trunk and head.
    The main idea here is to train multiple instances of the head with the same trunk. For example, a trunk can be feature extraction layers in a deep classification network, and the head can be the classification layers.
    Since the primary motivation of the subensembles is to improve the training speed, there is a minute drop in performance compared to Deep Ensembles as it is a tradeoff between the computational time and quality of uncertainty.
    Masksembles proposed in \cite{masksembles} are relatively new and combine Deep Ensembles with MC-Dropout.
    Masksembles also propose a predefined mask for each layer, and only those specific neurons are dropped every time.

    Other methods include snapshot ensembles \cite{snapshot_ensembles} which iterate over multiple local optima in the optimization landscape using a cyclic learning rate. 
    The model parameters are saved at each of the local optima. 
    All these flavours of Deep Ensembles are proposed to reduce the effect on time.
    The other problem with snapshot ensembles is that the optimization landscape in deep neural networks is poorly studied, so one cannot say with certainty that the model saved at two local minima is uncorrelated.
    A neural ensemble search also exists \cite{NAS_Ensembles}, where the neural architecture search is applied over the Ensembles.
    Instead of using various instances of the same model, authors in \cite{NAS_Ensembles} use various instances of various models in the architecture search space.
    In this thesis, we chose to use Deep Ensembles because of ease of implementation and Deep Ensembles also act as a baseline comparison for other nsemble methods in the future.

    \subsection{Bayesian Methods}
    Existing neural networks are trained in maximum likelihood, resulting in point estimates for the weights.
    The main idea behind Bayesian neural networks is to use a distribution over the network parameters.
    Instead of a single fixed weight tensor for a layer in a neural network, a weight tensor is drawn from the distribution for each forward pass.
    The parameters are estimated for input during training by using Bayes rule and expressed in Equation~\ref{eq:bayes_1}.
    \begin{equation}
        p(\theta|x, y) = \frac{p(y|x, \theta)p(\theta)}{p(y|x)} = \frac{p(y|x, \theta)p(\theta)}{\int p(y|x, \theta) p(\theta) d\theta} \label{eq:bayes_1}
    \end{equation}
    Here $\theta$ represents network parameters (weights), $p(\theta)$ represents prior distribution over $\theta$, and x and y represent the input data. In our case, x represents the input point cloud and y its corresponding semantic point labels.
    During inference, the labels are calculated by Bayesian model averaging as given in below Equation~\ref{eq:bayes_2}.
    \begin{equation}
        p(y_t|x_t, x, y) = \int p(y_t|x_t, \theta)p(\theta|x, y)d\theta \label{eq:bayes_2}  
    \end{equation}
    Here $\theta$ represents trained network parameters, x and y represent the training set, and $x_t$ and $y_t$ represent the test set.
    The integrals in Equations~\ref{eq:bayes_1} and \ref{eq:bayes_2} cannot be computed because $\theta$ is the continuous space and so iterating over all possible values of $\theta$ is not feasible.
    So to achieve tractable $\theta$, various approximation methods exist, such as Variational Inference (VI), Laplace approximation and Monte Carlo sampling.

    VI is an approximation method where the posterior probability $p(\theta|x, y)$ is approximated by specific distributions represented by $q(\theta)$.
    Kullback-Liebler Divergence (KLD) is used as a measure to calculate the difference between the two distributions. 
    Since KLD cannot be optimized directly because of the posterior distribution, a function called Evidence Lower BOund (ELBO) similar to KLD is proposed.
    \cite{Gaussian_Priors} represents $q(\theta)$ as a Gaussian approximation and \cite{weight_uncertainty} proposed Bayes by backprop to extend stochastic VI for non-gaussian priors.
    \cite{Non_Gaussian_Priors} provides a stochastic variational inference using ELBO loss over mini-batch of data and assumed the network parameter priors are Gaussian.
    \cite{Flipout} makes use of the Reparameterization trick for the reduction of variance in gradients. The Reparameterization trick led to the reformulation of ELBO loss which made it compatible with standard backpropagation.

    Another widely known example for VI is Monte Carlo Dropout (MC-Dropout), in which the dropout layers are reformulated as random variables with Bernoulli distribution.
    As in \cite{bhandary2020evaluating} and \cite{gawlikowski2021survey} training with dropout layers and predictive uncertainty can be calculated by applying MC-Dropout during inference..
    The other flavour of MC-Dropout is that the random incoming activations to the node are dropped instead of the nodes themselves, and this method is called Monte Carlo Dropconnect (MC-Dropconnect) as proposed in \cite{gawlikowski2021survey}.
    An example of MC-Dropout and MC-Dropconnect is depicted in Figure~\ref{fig:Dropout_Connect}.
    \begin{figure}[htbp]
        \begin{subfigure}{0.33\textwidth}
            \centering
            \includegraphics[scale=0.33]{images/BaseNW_SOTA.png}
            \caption{}
        \end{subfigure}
        \begin{subfigure}{0.33\textwidth}
            \centering
            \includegraphics[scale=0.33]{images/Dropout_SOTA.png}
            \caption{}
            \label{fig:SOTA_Dropout}
        \end{subfigure}
        \begin{subfigure}{0.33\textwidth}
            \centering
            \includegraphics[scale=0.33]{images/DConnect_SOTA.png}
            \caption{}
            \label{fig:SOTA_Dconnect}
        \end{subfigure}
        \caption{Figure (a) depicting the connections of the regular fully connected layers with nodes and connections between them,
        (b) representing MC-Dropout where few nodes are dropped randomly and 
        (c) representing MC-Dropconnect where the weight connections are dropped randomly. All the images are taken from \cite{UQ_Survey}.}
        \label{fig:Dropout_Connect}
    \end{figure}
    Another method for estimating uncertainty is Flipout, as proposed in \cite{Flipout}.
    Although Flipout was proposed to decorrelate gradients in a minibatch by sampling independent weight perturbations for each example, it can be used for uncertainty estimation, and a detailed description is provided in Section~\ref{sec:meth_flipout}.
    
    % \textcolor{red}{Need refactoring a bit}
    % Sampling methods also called as Monte Carlo methods (not to be confused to Monte Carlo Dropout or Monte Carlo Dropconnect) estimate uncertainty without any approximation of parametric model.
    % The most popular method in the category of sampling methods is Markov Chain Monte Carlo (MCMC) sampling stated in \cite{Bishop_Book}.
    % Hamiltonian Monte Carlo (HMC) is an another variant of MCMC method and is considered as gold standard algorithm for Bayesian inference as stated in \cite{HMC}.
    % Laplace approximaiton methods make use of second order Tayler series approximation to estimate $p(\theta|x, y)$.
    % Uncertianty in the laplace approximation methods is calculated by taking Hessian matrix of $log(p(\theta|x, y))$ and can be applied to existing trained neural networks as in \cite{Laplace_approx}.
    % Sampling methods and laplace approimation methods arenot studied because the former is computationally expensive and later suffers from infeasibility to compute Hessian matrix for deep neural networks.
    
    
    
    \section{Out-of-Distribution detection}
    This section will discuss the difference between OOD, Anomaly using an example and then we continue with the existing OOD detection methods.

    \subsection{OOD/Anomaly/Distributional shift}
\label{sec:oodvanom}
\begin{figure}[h!]
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/old_ship.jpg}
        \caption{}
        \label{fig:old_ship}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/Trainer_cruiser.jpeg}
        \caption{}
        \label{fig:trian_cruiser}
    \end{subfigure}
%\end{figure}
%\begin{figure}[h!]
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/Anomaly_container.jpg}
        \caption{}
        \label{fig:anom_container}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/Anomaly_titanic.jpg}
        \caption{}
        \label{fig:anom_titanic}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/ood_submarine.jpg}
        \caption{}
        \label{fig:ood_submarine}
    \end{subfigure}
    \begin{subfigure}{0.333\textwidth}
        \centering
        \includegraphics[height=0.15\textheight,width=0.95\textwidth]{images/intro_ood_anomaly/ood_airship.jpg}
        \caption{}
        \label{fig:ood_airship}
    \end{subfigure}
    \caption{Illustration of Distributional Shift, Anomaly and Out-of-Distribution examples using various kind of ships. (a) represents the sail ship during 18th century. (b) depicts the current training data.
    (c), and (d) represents the anamolous ship data and (e), and (f) represents the OOD data. Images are taken from \cite{old_ship}, \cite{train_cruiser}, \cite{container},
    \cite{titanic}, \cite{submarine}, and \cite{airship} respectively in the order they appear.}
\end{figure}

Let us time travel back to $18^{th}$ century and assume that we had implemented a DNN model to detect ships.
The dataset images for training the DNN model will be similar to Figure~\ref{fig:old_ship}.
$18^{th}$ century ships as in Figure~\ref{fig:old_ship} can be defined as ``\textit{ships contain hull and sails}''.
Fast forward to the present time, current ships are as shown in Figure \ref{fig:trian_cruiser}.
Ship as in \ref{fig:trian_cruiser} can be defined as ``\textit{ships contain hull and passenger decks stacked upon each other}''.
Now, suppose we want to deploy the old model trained with old ships to detect
the present generation of ships. In that case, it is difficult because of the way we define ships.
This change in data distribution over a period of time is called ``\textit{Distributional Shift}'' of the data.
Distributional Shift can be resolved by retraining the model.

An Anomaly can be defined as the patterns that do not conform to the expected training behaviour as proposed in  \cite{anomaly_sec1_1}.
By this definition, Figure \ref{fig:anom_container} and Figure \ref{fig:anom_titanic} can be considered anomalies.
This is because Figure \ref{fig:anom_container} is a container ship looking similar to Figure \ref{fig:trian_cruiser}. Instead of passenger decks, we have containers stacked.
Figure \ref{fig:anom_titanic} is also an Anomaly because the Titanic also has a hull, passenger decks
and chimneys. These additional chimnies as features deviates this image from the definition of the ship
and can be considered an ``\textit{Anomaly}''.

In the case of OOD, the input is drawn from an unknown distribution of unknown data, which is not near to the training distribution.
Figure~\ref{fig:ood_submarine} and Figure~\ref{fig:ood_airship} are submarine, and an airship which does not adhere to the definition of the ship by any means.
So we regard these two images (submarine and airship) as OOD objects.


    \subsection{OOD Detection Methods}
    This subsection will discuss the existing OOD detection methods for 2D classification and 2D semantic segmentation tasks.
    To the best of our knowledge, ours is the first work to study OOD detection for the task of 3D semantic segmentation.

    The most widely used benchmark datasets for 2D classification are CIFAR10-vs-SVHN \cite{liang2017enhancing_ODIN}, CIFAR10-vs-LSUN \cite{hendrycks2016baseline_MSP}, and MNIST-vs-notMNIST \cite{hendrycks2016baseline_MSP}. 
    Most of the proposed methods for OOD detection in classification tasks are threshold-based methods which are posthoc methods.
    These methods employ a threshold-based detector and do not see the OOD data during training.
    The baseline method for threshold-based methods is proposed in \cite{hendrycks2016baseline_MSP}.
    \cite{hendrycks2016baseline_MSP} uses Maximum Softmax Probability (MSP) and argues that in-distribution dataset has a higher softmax score, and the out-of-distribution dataset has a lower softmax score.
    Since the softmax scores can be overconfident, \cite{liang2017enhancing_ODIN} proposed Out of DIstribution detector for Neural networks (ODIN), which makes use of calibrated softmax score by addition of temperature constant to softmax scores and computed as in Equation~\ref{eq:SODIN} where $f_i(x)$ is the output of the neural network and $T$ is the temperature constant for calibrated softmax scores.
    In addition to calibrated softmax scores, ODIN also adds noise perturbations to the input making the training adversarial.
    ODIN needs access to the OOD samples because the finetuning of perturbation magnitude is based on these samples.
    \begin{equation}
        S_{MSP}(x) = max_i \frac{exp(f_i(x))}{\sum^{C}_{j=1}exp(f_j(x))}  \label{eq:SMSP}
    \end{equation}
    \begin{equation}
        S_{ODIN}(x) = max_i \frac{exp(f_i(x)/T)}{\sum^{C}_{j=1}exp(f_j(x)/T)}  \label{eq:SODIN}
    \end{equation}
    % \begin{equation}
    %     \begin{cases}
    %         ID if S(x) > \delta_t \\
    %         OOD otherwise
    %     \end{cases}
    % \end{equation}
    \cite{lee2018simple_mahalanobis} has proposed a threshold-based OOD detection method using the Mahalanobis distance as a confidence score.
    The Mahalanobis distance is calculated for every activation map of each layer of the network, and these individual Mahalanobis scores are combined to get a confidence score.
    The Mahalanobis distance is calculated between the activation map and each class distribution represented as multivariate Gaussian.
    \cite{Maha_plus_ODIN} proposed the use of Mahalanobis distance and ODIN combined for OOD detection and argued that this combination yields better detection of OOD data.
    \cite{ReAct} proposed Rectified Activations (ReAct) for OOD detection wherein applied to the final layer of the neural network before softmax application, and it suppresses the higher activations to a constant, thereby limiting the effect of noise. 
    The ReAct operation is defined in Equation~\ref{eq:react} as
    \begin{equation}
        h^{-}(x) = ReAct(h(x); c) \text{ and } ReAct(x, c) = min(x, c) \label{eq:react}
    \end{equation} 
    where $h(x)$ is final layer activations, $c$ is the constant and $h^{-}(x)$ is ReAct output of $h(x)$.
    The score from this ReAct function is thresholded to detect the OOD data. 
    Recent developments in threshold-based models, as proposed in \cite{Energy_OOD_1} and \cite{Energy_OOD_2}, calculate energy value based on scalar value from logsumexponential operation over softmax scores.
    These energy scores are expected to be low for the In-Distribution (ID) dataset and vice versa for OOD datasets.

    The OOD data samples have higher uncertainty than ID samples, allowing us to use the uncertainty estimation methods for OOD detection.
    \cite{lakshminarayanan2016simple} proposes the Deep Ensembles to estimate epistemic uncertainty and utilizes this entropy from Deep Ensembles to classify ID-vs-OOD datasets.
    Similarly, \cite{JAmersfoot_RBF} proposed using the radial basis function to calculate the epistemic uncertainty and applied threshold over the epistemic uncertainty to classify OOD data.
    \cite{UOOD_BNN} proposes using the uncertainty estimated from Bayesian Neural Networks (BNNs) to detect OOD.
    It also argues that the choice of prior weights significantly affects the OOD performance.
    It also argues that the OOD detection performance using BNNs is marginally superior to other methods.
    \cite{Grad_UOOD} proposes the calculation of gradients during the inference and computes uncertainty using the calculated gradients, and argues that this approach is also comparably effective for OOD detection.
    \cite{UOOD_RL1} and \cite{UOOD_RL2} applies uncertainty to detect OOD data samples for the task of Deep Reinforcement Learning.
    
    In recent years, OOD detection for the task of 2D semantic segmentation has been in limelight.
    \cite{pixel_OOD} proposes the adaption of a few aforementioned OOD detection methods to the task of 2D semantic segmentation with few or no modifications.
    \cite{pixel_OOD} also proposed benchmarked datasets for pixel-level OOD detection and introduced a novel OOD detection evaluation metric called Max Intersection-over-Union (MaxIoU).
    \cite{SemSeg_Entropy1}, and \cite{SemSeg_Entropy2} proposed using the entropy values for OOD detection in the 2D semantic segmentation task.
    \cite{MetaSeg} proposed a small fully connected neural network called \textit{MetaSeg} with inputs being the metrics such as entropy, probability margin, meanIoU and variation ratio to classify ID and OOD samples.
    \cite{uncertianty_distillation} proposed uncertainty distillation to compute uncertainty estimates similar to knowledge distillation and argues these uncertainty estimates can be efficiently used for OOD detection.
    From the above discussed OOD methods, we use threshold based methods such as MSP and uncertainty scores because of their ease of implementation being post-hoc methods.
    They are also certain to deliver promising performance on OOD detection in most of the cases.
    

\end{document}
